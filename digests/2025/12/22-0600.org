#+TITLE: HN Digest 2025-12-22 06:00:00 UT UTC
#+DATE: 2025-12-22T06:00:00Z
#+CURATOR: claude
#+SOURCE: https://hacker-news.firebaseio.com/v0/

* Vibe
Compilers for fun, shutdowns for real, and Dijkstra's ghost judges your code from beyond

* Highlights
- Joy of compilers: Programming should be fun, not a competition
- 296 shutdowns: Governments keep flipping the internet kill switch
- Dijkstra archive: Thousands of handwritten manuscripts now online
- Autograd.c: PyTorch rebuilt from scratch in 700 lines of C
- Silent FP16: Your ML model might be lying about its precision

* Stories

** I'm just having fun :programming:career:compilers:philosophy:
:PROPERTIES:
:ID:       46287253
:URL:      https://jyn.dev/i-m-just-having-fun/
:HN_URL:   https://news.ycombinator.com/item?id=46287253
:POINTS:   266
:COMMENTS: 85
:BY:       lemper
:END:

*** TLDR
A compiler engineer argues that programming should be about curiosity and experimentation, not competition. Build tools that make you happy, start small, and don't be intimidated by complex projects. Learning happens through play, not through proving you're the smartest person in the room.

*** Take
Finally, someone said it. Half the industry is miserable because they forgot this was supposed to be fun. The other half is on LinkedIn bragging about their 'passion' while secretly hating every sprint.

*** Comments

**** nanolith
:PROPERTIES:
:COMMENT_ID: 46287301
:END:
We need more people willing to do their own thing, even if others find it intimidating or silly. Sometimes these things become real businesses.

**** com2kid
:PROPERTIES:
:COMMENT_ID: 46287445
:END:
I joined a compiler team out of college because it seemed like fun. Went from C# to embedded engineering to running a startup in React. Curiosity is the real skill.

*** i18n                                                  :i18n:

**** zh
:PROPERTIES:
:LANG: zh
:END:
***** TLDR
一位编译器工程师认为编程应该是关于好奇心和实验的，而不是竞争。构建让你开心的工具，从小处开始，不要被复杂的项目吓倒。学习来自于玩耍，而不是证明你是房间里最聪明的人。
***** Take
终于有人说出来了。这个行业一半人不开心，因为他们忘了这本来应该是好玩的。另一半在领英上吹嘘他们的'热情'，但内心讨厌每一个冲刺。
**** ja
:PROPERTIES:
:LANG: ja
:END:
***** TLDR
コンパイラエンジニアが、プログラミングは競争ではなく、好奇心と実験であるべきだと主張している。自分を幸せにするツールを作り、小さく始め、複雑なプロジェクトに怖気づかないこと。学びは遊びから生まれる。
***** Take
やっと誰かが言ってくれた。業界の半分は、これが楽しいはずだったことを忘れて不幸になっている。残りの半分はLinkedInで「情熱」を自慢しながら、密かにすべてのスプリントを嫌っている。
**** ko
:PROPERTIES:
:LANG: ko
:END:
***** TLDR
컴파일러 엔지니어가 프로그래밍은 경쟁이 아니라 호기심과 실험에 관한 것이어야 한다고 주장합니다. 자신을 행복하게 하는 도구를 만들고, 작게 시작하고, 복잡한 프로젝트에 겁먹지 마세요. 배움은 놀이에서 나옵니다.
***** Take
드디어 누가 말했네요. 업계 절반은 이게 재미있어야 한다는 걸 잊어서 불행합니다. 나머지 절반은 LinkedIn에서 '열정'을 자랑하면서 몰래 모든 스프린트를 싫어합니다.
**** es
:PROPERTIES:
:LANG: es
:END:
***** TLDR
Un ingeniero de compiladores argumenta que la programacion deberia ser sobre curiosidad y experimentacion, no competencia. Construye herramientas que te hagan feliz, empieza pequeno y no te intimides por proyectos complejos. El aprendizaje viene del juego.
***** Take
Por fin alguien lo dijo. La mitad de la industria es miserable porque olvidaron que esto deberia ser divertido. La otra mitad esta en LinkedIn presumiendo su 'pasion' mientras secretamente odian cada sprint.
**** de
:PROPERTIES:
:LANG: de
:END:
***** TLDR
Ein Compiler-Ingenieur argumentiert, dass Programmieren Neugier und Experimentieren sein sollte, nicht Wettbewerb. Baue Werkzeuge die dich gluecklich machen, fang klein an und lass dich nicht von komplexen Projekten einschuechtern. Lernen kommt durch Spielen.
***** Take
Endlich hat es jemand gesagt. Die halbe Industrie ist ungluecklich, weil sie vergessen haben, dass das Spass machen sollte. Die andere Haelfte prahlt auf LinkedIn mit ihrer 'Leidenschaft' waehrend sie heimlich jeden Sprint hassen.
** Deliberate Internet Shutdowns :security:privacy:infrastructure:politics:
:PROPERTIES:
:ID:       46316050
:URL:      https://www.schneier.com/blog/archives/2025/12/deliberate-internet-shutdowns.html
:HN_URL:   https://news.ycombinator.com/item?id=46316050
:POINTS:   110
:COMMENTS: 34
:BY:       WaitWaitWha
:END:

*** TLDR
Bruce Schneier reports 296 deliberate government internet shutdowns across 54 countries in 2024, with 244 more already in 2025. Motivations range from suppressing protests to blocking exam cheating. As internet becomes critical infrastructure for banking, emergency services, and daily life, these shutdowns cause increasingly severe collateral damage.

*** Take
Every time a government shuts down the internet 'for security', a banker somewhere loses access to their trading platform. Strange how that never seems to factor into the decision.

*** Comments

**** stego-tech
:PROPERTIES:
:COMMENT_ID: 46316123
:END:
The post is mainly a call to action against internet centralization and government control of core infrastructure. We need more examples of harms for folks to draw on.

**** modeless
:PROPERTIES:
:COMMENT_ID: 46316234
:END:
I thought this would be advocating 'chaos monkey' style intentional shutdown to test resiliency. Might not be a bad idea - maybe once every four years on leap day.

*** i18n                                                  :i18n:

**** zh
:PROPERTIES:
:LANG: zh
:END:
***** TLDR
Bruce Schneier报告说，2024年54个国家发生了296次政府故意关闭互联网事件，2025年已经有244次。动机从镇压抗议到阻止考试作弊不等。随着互联网成为银行、紧急服务和日常生活的关键基础设施，这些关闭造成的附带损害越来越严重。
***** Take
每次政府以'安全'为由关闭互联网，某个银行家就会失去交易平台的访问权。奇怪的是，这似乎从来不是决策的考虑因素。
**** ja
:PROPERTIES:
:LANG: ja
:END:
***** TLDR
Bruce Schneierは、2024年に54カ国で296件の政府による意図的なインターネット遮断があり、2025年にはすでに244件あると報告している。動機は抗議活動の抑制から試験のカンニング防止まで様々。インターネットが銀行、緊急サービス、日常生活の重要なインフラになるにつれ、これらの遮断による付随的被害はますます深刻になっている。
***** Take
政府が「セキュリティのため」にインターネットを遮断するたびに、どこかの銀行員が取引プラットフォームへのアクセスを失う。不思議なことに、それは決定に考慮されないようだ。
**** ko
:PROPERTIES:
:LANG: ko
:END:
***** TLDR
Bruce Schneier는 2024년 54개국에서 296건의 정부 의도적 인터넷 차단이 있었고, 2025년에는 이미 244건이 있다고 보고합니다. 동기는 시위 진압부터 시험 부정행위 차단까지 다양합니다. 인터넷이 은행, 응급 서비스, 일상 생활의 핵심 인프라가 되면서 이러한 차단으로 인한 부수적 피해가 점점 심각해지고 있습니다.
***** Take
정부가 '보안을 위해' 인터넷을 차단할 때마다 어딘가의 은행원이 거래 플랫폼 접근을 잃습니다. 이상하게도 그것은 결정에 고려되지 않는 것 같습니다.
**** es
:PROPERTIES:
:LANG: es
:END:
***** TLDR
Bruce Schneier reporta 296 apagones deliberados de internet por gobiernos en 54 paises en 2024, con 244 mas ya en 2025. Las motivaciones van desde suprimir protestas hasta bloquear trampas en examenes. A medida que internet se convierte en infraestructura critica para bancos, servicios de emergencia y vida diaria, estos apagones causan danos colaterales cada vez mas severos.
***** Take
Cada vez que un gobierno apaga internet 'por seguridad', un banquero pierde acceso a su plataforma de trading. Extrano que eso nunca parece ser un factor en la decision.
**** de
:PROPERTIES:
:LANG: de
:END:
***** TLDR
Bruce Schneier berichtet ueber 296 absichtliche Regierungs-Internet-Abschaltungen in 54 Laendern im Jahr 2024, mit 244 weiteren bereits 2025. Motivationen reichen von Protestunterdrueckung bis zur Verhinderung von Pruefungsbetrug. Da Internet zur kritischen Infrastruktur fuer Banken, Notdienste und den Alltag wird, verursachen diese Abschaltungen zunehmend schwere Kollateralschaeden.
***** Take
Jedes Mal wenn eine Regierung das Internet 'aus Sicherheitsgruenden' abschaltet, verliert irgendwo ein Banker den Zugang zu seiner Handelsplattform. Seltsam, dass das nie in die Entscheidung einfliesst.
** E.W. Dijkstra Archive :history:computer-science:algorithms:programming:
:PROPERTIES:
:ID:       46345523
:URL:      https://www.cs.utexas.edu/~EWD/welcome.html
:HN_URL:   https://news.ycombinator.com/item?id=46345523
:POINTS:   127
:COMMENTS: 12
:BY:       surprisetalk
:END:

*** TLDR
The University of Texas hosts a comprehensive archive of Edsger Dijkstra's handwritten manuscripts - over a thousand consecutively numbered technical notes spanning his 40-year career. The archive includes PDFs, transcriptions, translations, and cross-references documenting his foundational work in algorithms, programming languages, and distributed computing.

*** Take
Somewhere in there is a handwritten note explaining why your code is garbage. Dijkstra would have had opinions about your variable naming.

*** Comments

**** coderatlarge
:PROPERTIES:
:COMMENT_ID: 46345612
:END:
What a charming time it was when that generation discovered a bunch of stuff that now undergirds daily life. Dijkstra always believed it a scientist's duty to maintain lively correspondence with colleagues.

**** anonzzzies
:PROPERTIES:
:COMMENT_ID: 46345734
:END:
I met the man a few times. Both my father and him always told me to not just write code, but proofs first. I'm rather happy he's not alive with this LLM stuff. He would've considered it the worst thing ever.

*** i18n                                                  :i18n:

**** zh
:PROPERTIES:
:LANG: zh
:END:
***** TLDR
德克萨斯大学托管了Edsger Dijkstra手写手稿的综合档案——超过一千份连续编号的技术笔记，跨越他40年的职业生涯。档案包括PDF、转录、翻译和交叉引用，记录了他在算法、编程语言和分布式计算方面的基础性工作。
***** Take
里面某处有一张手写便条解释为什么你的代码是垃圾。Dijkstra会对你的变量命名有意见的。
**** ja
:PROPERTIES:
:LANG: ja
:END:
***** TLDR
テキサス大学がEdsger Dijkstraの手書き原稿の包括的なアーカイブをホストしている - 40年のキャリアにわたる1000以上の連番技術ノート。アーカイブにはPDF、転写、翻訳、相互参照が含まれ、アルゴリズム、プログラミング言語、分散コンピューティングにおける彼の基礎的な仕事を記録している。
***** Take
どこかにあなたのコードがゴミである理由を説明する手書きのメモがある。Dijkstraはあなたの変数命名について意見を持っていただろう。
**** ko
:PROPERTIES:
:LANG: ko
:END:
***** TLDR
텍사스 대학교가 Edsger Dijkstra의 손으로 쓴 원고에 대한 포괄적인 아카이브를 호스팅합니다 - 40년 경력에 걸친 천 개 이상의 연속 번호가 매겨진 기술 노트. 아카이브에는 PDF, 전사, 번역, 상호 참조가 포함되어 알고리즘, 프로그래밍 언어, 분산 컴퓨팅에 대한 그의 기초적인 작업을 문서화합니다.
***** Take
그 어딘가에 당신의 코드가 쓰레기인 이유를 설명하는 손으로 쓴 메모가 있습니다. Dijkstra는 당신의 변수 이름에 대해 의견이 있었을 것입니다.
**** es
:PROPERTIES:
:LANG: es
:END:
***** TLDR
La Universidad de Texas aloja un archivo completo de los manuscritos escritos a mano de Edsger Dijkstra - mas de mil notas tecnicas numeradas consecutivamente a lo largo de su carrera de 40 anos. El archivo incluye PDFs, transcripciones, traducciones y referencias cruzadas documentando su trabajo fundamental en algoritmos, lenguajes de programacion y computacion distribuida.
***** Take
En algun lugar ahi hay una nota escrita a mano explicando por que tu codigo es basura. Dijkstra habria tenido opiniones sobre tus nombres de variables.
**** de
:PROPERTIES:
:LANG: de
:END:
***** TLDR
Die University of Texas beherbergt ein umfassendes Archiv von Edsger Dijkstras handgeschriebenen Manuskripten - ueber tausend durchnummerierte technische Notizen aus seiner 40-jaehrigen Karriere. Das Archiv enthaelt PDFs, Transkriptionen, Uebersetzungen und Querverweise, die seine grundlegende Arbeit in Algorithmen, Programmiersprachen und verteiltem Rechnen dokumentieren.
***** Take
Irgendwo dort drin ist eine handgeschriebene Notiz, die erklaert, warum dein Code Muell ist. Dijkstra haette Meinungen zu deiner Variablenbenennung gehabt.
** Show HN: Autograd.c - A tiny ML framework built from scratch :ml:c:opensource:education:
:PROPERTIES:
:ID:       46285424
:URL:      https://github.com/sueszli/autograd.c
:HN_URL:   https://news.ycombinator.com/item?id=46285424
:POINTS:   71
:COMMENTS: 8
:BY:       sueszli
:END:

*** TLDR
A minimal reverse-mode automatic differentiation engine in C - essentially PyTorch rebuilt in ~700 lines. Features reference-counted tensors, arena-allocated function nodes, and centralized gradient accumulation. The author trained it on CIFAR-10 achieving 23.56% accuracy, proving it actually works.

*** Take
Nothing says 'I understand ML at a fundamental level' like rewriting gradient descent in C. The 23% accuracy is honest - most researchers would have cherry-picked a different metric.

*** Comments

**** sueszli
:PROPERTIES:
:COMMENT_ID: 46285501
:END:
Woah, this got way more attention than I expected. If you're interested in technical details, the design specs are in docs. If you're working on similar mlsys projects, please reach out.

**** spwa4
:PROPERTIES:
:COMMENT_ID: 46285623
:END:
Cool. But this makes me wonder - is there a compiler-autograd 'library'? Something that would compile into C to execute as fast as possible on CPUs with no indirection at all.

*** i18n                                                  :i18n:

**** zh
:PROPERTIES:
:LANG: zh
:END:
***** TLDR
一个用C语言实现的最小反向模式自动微分引擎——本质上是用约700行代码重建的PyTorch。特性包括引用计数张量、arena分配的函数节点和集中式梯度累积。作者在CIFAR-10上训练，达到23.56%的准确率，证明它确实有效。
***** Take
没有什么比用C重写梯度下降更能说明'我从根本上理解ML'了。23%的准确率很诚实——大多数研究人员会挑选一个不同的指标。
**** ja
:PROPERTIES:
:LANG: ja
:END:
***** TLDR
Cで実装された最小限の逆モード自動微分エンジン - 本質的に約700行で再構築されたPyTorch。参照カウントテンソル、アリーナ割り当て関数ノード、集中勾配蓄積を特徴とする。著者はCIFAR-10で23.56%の精度を達成し、実際に動くことを証明した。
***** Take
Cで勾配降下法を書き直すこと以上に「MLを根本的に理解している」ことを示すものはない。23%の精度は正直だ - ほとんどの研究者は別の指標を選んでいただろう。
**** ko
:PROPERTIES:
:LANG: ko
:END:
***** TLDR
C로 구현된 최소한의 역방향 모드 자동 미분 엔진 - 본질적으로 약 700줄로 재구축된 PyTorch입니다. 참조 카운트 텐서, 아레나 할당 함수 노드, 중앙 집중식 그래디언트 축적을 특징으로 합니다. 저자는 CIFAR-10에서 23.56% 정확도를 달성하여 실제로 작동함을 증명했습니다.
***** Take
C로 경사 하강법을 다시 작성하는 것만큼 'ML을 근본적으로 이해한다'는 것을 보여주는 것은 없습니다. 23% 정확도는 솔직합니다 - 대부분의 연구자들은 다른 지표를 선택했을 것입니다.
**** es
:PROPERTIES:
:LANG: es
:END:
***** TLDR
Un motor minimo de diferenciacion automatica en modo reverso en C - esencialmente PyTorch reconstruido en ~700 lineas. Presenta tensores con conteo de referencias, nodos de funcion asignados en arena y acumulacion centralizada de gradientes. El autor lo entreno en CIFAR-10 logrando 23.56% de precision, probando que realmente funciona.
***** Take
Nada dice 'entiendo ML a nivel fundamental' como reescribir descenso de gradiente en C. La precision del 23% es honesta - la mayoria de investigadores habrian elegido una metrica diferente.
**** de
:PROPERTIES:
:LANG: de
:END:
***** TLDR
Eine minimale Reverse-Mode automatische Differenzierungs-Engine in C - im Wesentlichen PyTorch in ~700 Zeilen neu gebaut. Mit referenzgezaehlten Tensoren, Arena-allokierten Funktionsknoten und zentralisierter Gradientenakkumulation. Der Autor trainierte es auf CIFAR-10 und erreichte 23.56% Genauigkeit, was beweist, dass es funktioniert.
***** Take
Nichts sagt 'ich verstehe ML grundlegend' wie Gradient Descent in C neu zu schreiben. Die 23% Genauigkeit ist ehrlich - die meisten Forscher haetten eine andere Metrik gewaehlt.
** ONNX Runtime and CoreML May Silently Convert Your Model to FP16 :ml:apple:debugging:performance:
:PROPERTIES:
:ID:       46350075
:URL:      https://ym2132.github.io/ONNX_MLProgram_NN_exploration
:HN_URL:   https://news.ycombinator.com/item?id=46350075
:POINTS:   53
:COMMENTS: 8
:BY:       Two_hands
:END:

*** TLDR
ONNX Runtime's CoreML Execution Provider silently converts models to FP16 precision when using the older Neural Network format, causing inference discrepancies between CPU and GPU. The fix is simple: specify ModelFormat as 'MLProgram' to preserve original precision. The issue stems from CoreML's 2017-era NN format defaulting to FP16 for Apple's Neural Engine.

*** Take
Your ML pipeline has been lying to you this whole time and you never noticed because the numbers were close enough. Classic Apple: the default behavior is wrong but beautifully optimized.

*** Comments

**** yousifa
:PROPERTIES:
:COMMENT_ID: 46350123
:END:
On the CoreML side this is likely because the Neural Engine supports FP16 and offloading to ANE significantly increases inference time and power usage. You can inspect in Xcode profiler to see what's running at what precision.

**** DiabloD3
:PROPERTIES:
:COMMENT_ID: 46350234
:END:
This is why I laugh at so called 'AI researchers'. They build 'quality software' like this, while everyone else uses ggml and llama.cpp and doesn't have these weird issues.

*** i18n                                                  :i18n:

**** zh
:PROPERTIES:
:LANG: zh
:END:
***** TLDR
ONNX Runtime的CoreML执行提供程序在使用较旧的神经网络格式时会悄悄将模型转换为FP16精度，导致CPU和GPU之间的推理差异。修复很简单：将ModelFormat指定为'MLProgram'以保留原始精度。这个问题源于CoreML 2017年代的NN格式默认为Apple的神经引擎使用FP16。
***** Take
你的ML管道一直在骗你，你从来没注意到，因为数字够接近了。典型的Apple：默认行为是错的，但优化得很漂亮。
**** ja
:PROPERTIES:
:LANG: ja
:END:
***** TLDR
ONNX RuntimeのCoreML実行プロバイダは、古いNeural Network形式を使用するとモデルを黙ってFP16精度に変換し、CPUとGPU間の推論の不一致を引き起こす。修正は簡単：ModelFormatを'MLProgram'に指定して元の精度を保持する。この問題は、CoreMLの2017年代のNN形式がAppleのNeural Engine用にFP16をデフォルトにしていることに起因する。
***** Take
あなたのMLパイプラインはずっと嘘をついていて、数字が十分近かったから気づかなかった。典型的なApple：デフォルトの動作は間違っているが、美しく最適化されている。
**** ko
:PROPERTIES:
:LANG: ko
:END:
***** TLDR
ONNX Runtime의 CoreML 실행 제공자는 이전 Neural Network 형식을 사용할 때 모델을 조용히 FP16 정밀도로 변환하여 CPU와 GPU 간의 추론 불일치를 야기합니다. 수정은 간단합니다: ModelFormat을 'MLProgram'으로 지정하여 원래 정밀도를 유지하세요. 이 문제는 CoreML의 2017년 NN 형식이 Apple Neural Engine용으로 FP16을 기본값으로 사용하는 데서 비롯됩니다.
***** Take
당신의 ML 파이프라인은 계속 거짓말을 해왔고 숫자가 충분히 가까웠기 때문에 눈치채지 못했습니다. 전형적인 Apple: 기본 동작은 틀렸지만 아름답게 최적화되어 있습니다.
**** es
:PROPERTIES:
:LANG: es
:END:
***** TLDR
El proveedor de ejecucion CoreML de ONNX Runtime convierte silenciosamente los modelos a precision FP16 cuando usa el formato Neural Network antiguo, causando discrepancias de inferencia entre CPU y GPU. La solucion es simple: especifica ModelFormat como 'MLProgram' para preservar la precision original. El problema viene del formato NN de CoreML de 2017 que usa FP16 por defecto para el Neural Engine de Apple.
***** Take
Tu pipeline de ML te ha estado mintiendo todo este tiempo y nunca lo notaste porque los numeros eran suficientemente cercanos. Clasico Apple: el comportamiento por defecto esta mal pero bellamente optimizado.
**** de
:PROPERTIES:
:LANG: de
:END:
***** TLDR
ONNX Runtimes CoreML Execution Provider konvertiert Modelle heimlich zu FP16-Praezision bei Verwendung des aelteren Neural Network Formats, was zu Inferenz-Diskrepanzen zwischen CPU und GPU fuehrt. Die Loesung ist einfach: ModelFormat als 'MLProgram' angeben um die urspruengliche Praezision zu erhalten. Das Problem stammt vom CoreML NN-Format aus 2017, das FP16 als Standard fuer Apples Neural Engine verwendet.
***** Take
Deine ML-Pipeline hat dich die ganze Zeit angelogen und du hast es nie bemerkt, weil die Zahlen nah genug waren. Typisch Apple: Das Standardverhalten ist falsch aber wunderschoen optimiert.