#+TITLE: HN Digest 2026-01-31 03:00:00 UT UTC
#+DATE: 2026-01-31T03:00:00Z
#+CURATOR: claude
#+SOURCE: https://hacker-news.firebaseio.com/v0/

* Vibe
Nvidia ghosts OpenAI, a 9M model schools your tones, and AI joins a social network

* Highlights
- OpenAI-Nvidia $100B deal goes cold as market shifts
- 9M-parameter model grades your Mandarin pronunciation in-browser
- Kimi K2.5 makes open source believers out of skeptics

* Stories

** The $100B megadeal between OpenAI and Nvidia is on ice :ai:nvidia:openai:business:
:PROPERTIES:
:ID:       46831702
:URL:      https://www.wsj.com/tech/ai/the-100-billion-megadeal-between-openai-and-nvidia-is-on-ice-aa3025e3
:HN_URL:   https://news.ycombinator.com/item?id=46831702
:POINTS:   171
:COMMENTS: 83
:BY:       pixelesque
:END:

*** TLDR
［Article paywalled］ Per HN comments: OpenAI's market share has declined significantly in the past 6 months while Nvidia has been training its own models. The strategic alliance that made sense before now looks like a liability. Altman's deposition revelations about duplicitous behavior may have contributed.

*** Take
When you're training your own models and your potential partner is bleeding market share, suddenly $100B feels less like an investment and more like charity.

*** Comments

**** jjcm
:PROPERTIES:
:COMMENT_ID: 46831800
:END:
Not only has OpenAI's market share gone down significantly in the last 6mo, Nvidia has been using its newfound liquid funds to train its own family of models. An alliance with OpenAI just makes less sense today than it did 6mo ago.

**** mordymoop
:PROPERTIES:
:COMMENT_ID: 46831850
:END:
I wonder how much the indications of Altman's duplicitous behavior through the deposition findings have been relevant here.

*** i18n                                                  :i18n:

**** zh
:PROPERTIES:
:LANG: zh
:END:
***** Title
OpenAI与Nvidia的千亿美元交易陷入僵局
***** TLDR
据HN评论：OpenAI过去6个月市场份额大幅下滑，而Nvidia一直在训练自己的模型。曾经合理的战略联盟现在看起来像是负担。
***** Take
当你在训练自己的模型，而潜在合作伙伴市场份额在流失时，1000亿美元突然感觉不像投资，更像慈善。
***** Comments
- OpenAI市场份额大幅下降，Nvidia用新资金训练自己的模型。联盟不再有意义。
- 不知道Altman在证词中暴露的两面性行为对此有多大影响。
**** ja
:PROPERTIES:
:LANG: ja
:END:
***** Title
OpenAIとNvidiaの1000億ドル大型契約が凍結
***** TLDR
HNコメントより：OpenAIの市場シェアは過去6ヶ月で大幅に減少し、Nvidiaは独自モデルの訓練を進めている。かつて理にかなっていた戦略的提携は今や負債のように見える。
***** Take
自社モデルを訓練中で、パートナー候補が市場シェアを失っているとき、1000億ドルは投資というより慈善事業に感じる。
***** Comments
- OpenAIのシェアが大幅減少し、Nvidiaは自社モデルを訓練中。提携は今や意味がない。
- アルトマンの二枚舌な行動が証言で明らかになったことが関係しているのかな。
**** ko
:PROPERTIES:
:LANG: ko
:END:
***** Title
OpenAI와 Nvidia 간 1000억 달러 대형 거래 중단
***** TLDR
HN 댓글에 따르면: OpenAI의 시장 점유율이 지난 6개월간 크게 하락했고, Nvidia는 자체 모델 훈련에 자금을 투입 중이다. 한때 합리적이었던 전략적 동맹이 이제는 부담으로 보인다.
***** Take
자체 모델을 훈련하면서 잠재 파트너가 시장 점유율을 잃고 있을 때, 1000억 달러는 투자가 아니라 자선사업처럼 느껴진다.
***** Comments
- OpenAI 시장점유율이 크게 하락하고 Nvidia는 자체 모델 훈련 중. 동맹은 더 이상 의미가 없다.
- 알트만의 이중적 행동이 증언에서 드러난 것이 영향을 미쳤을지 궁금하다.
**** es
:PROPERTIES:
:LANG: es
:END:
***** Title
El megatrato de $100B entre OpenAI y Nvidia está en pausa
***** TLDR
Según comentarios de HN: la cuota de mercado de OpenAI ha caído significativamente en los últimos 6 meses mientras Nvidia entrena sus propios modelos. La alianza estratégica que antes tenía sentido ahora parece una carga.
***** Take
Cuando entrenas tus propios modelos y tu potencial socio está perdiendo cuota de mercado, $100B de repente se siente menos como inversión y más como caridad.
***** Comments
- La cuota de OpenAI cayó mucho y Nvidia entrena sus propios modelos. La alianza ya no tiene sentido.
- Me pregunto cuánto influyeron las revelaciones sobre el comportamiento duplicitoso de Altman.
**** de
:PROPERTIES:
:LANG: de
:END:
***** Title
Der 100-Milliarden-Dollar-Deal zwischen OpenAI und Nvidia liegt auf Eis
***** TLDR
Laut HN-Kommentaren: OpenAIs Marktanteil ist in den letzten 6 Monaten deutlich gesunken, während Nvidia eigene Modelle trainiert. Die strategische Allianz, die früher sinnvoll war, wirkt jetzt wie eine Last.
***** Take
Wenn du deine eigenen Modelle trainierst und dein potenzieller Partner Marktanteile verliert, fühlen sich 100 Milliarden plötzlich weniger wie eine Investition und mehr wie Wohltätigkeit an.
***** Comments
- OpenAIs Marktanteil ist stark gesunken und Nvidia trainiert eigene Modelle. Die Allianz ergibt keinen Sinn mehr.
- Ich frage mich, wie sehr Altmans doppelzüngiges Verhalten aus den Aussagen hier eine Rolle spielt.
** Show HN: I trained a 9M speech model to fix my Mandarin tones :ml:mandarin:speech:show-hn:
:PROPERTIES:
:ID:       46832074
:URL:      https://simedw.com/2026/01/31/ear-pronunication-via-ctc/
:HN_URL:   https://news.ycombinator.com/item?id=46832074
:POINTS:   99
:COMMENTS: 29
:BY:       simedw
:END:

*** TLDR
A 9M-parameter Conformer-CTC model trained on ~300 hours of Mandarin speech (AISHELL + Primewords) that grades pronunciation and tones syllable-by-syllable. Quantized to 11MB INT8, runs entirely in-browser via ONNX Runtime Web. Uses Viterbi forced alignment for feedback.

*** Take
When native speakers complain they have to over-enunciate to pass, you know you've built a strict teacher. Finally, a model that won't politely lie about your terrible tones.

*** Comments

**** stuxnet79
:PROPERTIES:
:COMMENT_ID: 46832100
:END:
How difficult would it be to adapt this to Cantonese? It is a surprisingly difficult language to learn. It has more tones than Mandarin plus comparatively less access to learning resources.

**** ecshafer
:PROPERTIES:
:COMMENT_ID: 46832150
:END:
Anyone that is a native European language speaker that hasn't tried to learn Chinese, it's really hard to understand how hard it is. The tones can be very subtle, and your ear is not fine tuned to them.

*** i18n                                                  :i18n:

**** zh
:PROPERTIES:
:LANG: zh
:END:
***** Title
Show HN：我训练了一个900万参数的语音模型来纠正我的普通话声调
***** TLDR
一个900万参数的Conformer-CTC模型，在约300小时普通话语音数据上训练，逐音节评分发音和声调。量化到11MB INT8，通过ONNX Runtime Web完全在浏览器中运行。
***** Take
当母语者抱怨必须过度发音才能通过时，你就知道你建了一个严格的老师。终于有个模型不会对你糟糕的声调客气地撒谎了。
***** Comments
- 把这个适配到粤语有多难？粤语声调比普通话多，学习资源也更少。
- 没学过中文的欧洲语言母语者很难理解这有多难。声调非常微妙，你的耳朵没有调整好。
**** ja
:PROPERTIES:
:LANG: ja
:END:
***** Title
Show HN：中国語の声調を直すために900万パラメータの音声モデルを訓練した
***** TLDR
約300時間の中国語音声データで訓練された900万パラメータのConformer-CTCモデル。音節ごとに発音と声調を採点。11MB INT8に量子化され、ONNX Runtime Webでブラウザ内で完全に動作。
***** Take
ネイティブスピーカーが合格するには大げさに発音しないといけないと文句を言うなら、厳しい先生を作ったということ。やっと酷い声調について丁寧に嘘をつかないモデルができた。
***** Comments
- これを広東語に適用するのはどれくらい難しい？広東語は声調が多く、学習リソースも少ない。
- 中国語を学んだことない欧州言語話者には難しさが分からない。声調は微妙で、耳が調整されていない。
**** ko
:PROPERTIES:
:LANG: ko
:END:
***** Title
Show HN: 중국어 성조 교정을 위해 900만 파라미터 음성 모델을 훈련했습니다
***** TLDR
약 300시간의 중국어 음성 데이터로 훈련된 900만 파라미터 Conformer-CTC 모델. 음절별로 발음과 성조를 채점. 11MB INT8로 양자화되어 ONNX Runtime Web으로 브라우저에서 완전히 실행.
***** Take
원어민이 통과하려면 과장되게 발음해야 한다고 불평하면, 엄격한 선생을 만든 것이다. 드디어 형편없는 성조에 대해 예의 바르게 거짓말하지 않는 모델이 생겼다.
***** Comments
- 광동어에 적용하려면 얼마나 어려울까? 광동어는 성조가 더 많고 학습 자료도 적다.
- 중국어를 배워본 적 없는 유럽어 원어민은 얼마나 어려운지 이해하기 힘들다. 성조가 매우 미묘하다.
**** es
:PROPERTIES:
:LANG: es
:END:
***** Title
Show HN: Entrené un modelo de voz de 9M para corregir mis tonos en mandarín
***** TLDR
Un modelo Conformer-CTC de 9M parámetros entrenado con ~300 horas de habla en mandarín que califica pronunciación y tonos sílaba por sílaba. Cuantizado a 11MB INT8, funciona completamente en el navegador vía ONNX Runtime Web.
***** Take
Cuando los hablantes nativos se quejan de que tienen que sobre-pronunciar para aprobar, sabes que has construido un profesor estricto. Por fin un modelo que no mentirá cortésmente sobre tus tonos terribles.
***** Comments
- ¿Qué tan difícil sería adaptar esto al cantonés? Tiene más tonos que el mandarín y menos recursos de aprendizaje.
- Cualquier hablante nativo de lenguas europeas que no haya intentado aprender chino no entiende lo difícil que es.
**** de
:PROPERTIES:
:LANG: de
:END:
***** Title
Show HN: Ich trainierte ein 9M-Sprachmodell, um meine Mandarin-Töne zu verbessern
***** TLDR
Ein 9M-Parameter Conformer-CTC-Modell, trainiert auf ~300 Stunden Mandarin-Sprache, das Aussprache und Töne Silbe für Silbe bewertet. Auf 11MB INT8 quantisiert, läuft vollständig im Browser via ONNX Runtime Web.
***** Take
Wenn Muttersprachler sich beschweren, dass sie überdeutlich aussprechen müssen, um zu bestehen, hast du einen strengen Lehrer gebaut. Endlich ein Modell, das nicht höflich über deine schrecklichen Töne lügt.
***** Comments
- Wie schwierig wäre es, das auf Kantonesisch anzupassen? Es hat mehr Töne als Mandarin und weniger Lernressourcen.
- Jeder europäische Muttersprachler, der kein Chinesisch gelernt hat, versteht nicht, wie schwer es ist.
** Kimi K2.5 Technical Report :ai:open-source:ml:coding:
:PROPERTIES:
:ID:       46826597
:URL:      https://github.com/MoonshotAI/Kimi-K2.5/blob/master/tech_report.pdf
:HN_URL:   https://news.ycombinator.com/item?id=46826597
:POINTS:   236
:COMMENTS: 96
:BY:       vinhnx
:END:

*** TLDR
Moonshot AI releases Kimi K2.5, an open-weight model that users report performs on par with top proprietary models for coding. Praised for instruction following, staying on task, and avoiding context poisoning. Some note it lost the personality quirks from K2.

*** Take
The moment open source models make people 'almost hesitant to say it's as good as Opus' is the moment the proprietary moat starts looking more like a puddle.

*** Comments

**** zeroxfe
:PROPERTIES:
:COMMENT_ID: 46826650
:END:
I've been using this model as a coding agent for the past few days, and it's the first time I've felt that an open source model really competes with the big labs. I'm almost hesitant to say that this is as good as Opus.

**** logicprog
:PROPERTIES:
:COMMENT_ID: 46826700
:END:
It's so good at following my instructions, staying on task, and not getting context poisoned. It's definitely head and shoulders above the open weight competitors.

**** unleaded
:PROPERTIES:
:COMMENT_ID: 46826750
:END:
K2.5 has lost a lot of the personality from K2 unfortunately, talks in more ChatGPT/Gemini/C-3PO style now.

*** i18n                                                  :i18n:

**** zh
:PROPERTIES:
:LANG: zh
:END:
***** Title
Kimi K2.5 技术报告
***** TLDR
月之暗面发布Kimi K2.5开源权重模型，用户反馈其编码性能可与顶级闭源模型媲美。以指令遵循、专注任务和避免上下文污染著称。有人注意到它失去了K2的个性特点。
***** Take
当开源模型让人'几乎不敢说它和Opus一样好'时，专有护城河开始看起来更像一个水坑。
***** Comments
- 我用这个模型当编码助手几天了，这是第一次感觉开源模型真的能和大厂竞争。几乎不敢说它和Opus一样好。
- 它非常善于遵循指令、专注任务、不被上下文污染。明显优于其他开源竞品。
- K2.5失去了K2的很多个性，现在说话更像ChatGPT/Gemini风格了。
**** ja
:PROPERTIES:
:LANG: ja
:END:
***** Title
Kimi K2.5 技術レポート
***** TLDR
Moonshot AIがKimi K2.5オープンウェイトモデルをリリース。コーディングにおいてトップのプロプライエタリモデルと同等と報告されている。指示追従、タスク集中、コンテキスト汚染回避で評価。K2の個性が失われたとの声も。
***** Take
オープンソースモデルが「Opusと同等と言うのをためらう」レベルになったとき、プロプライエタリの堀は水たまりに見え始める。
***** Comments
- 数日間コーディングエージェントとして使っているが、初めてオープンソースが大手と競争できると感じた。Opusと同等と言うのをためらうほど。
- 指示追従、タスク集中、コンテキスト汚染回避が優秀。オープンウェイト競合より明らかに上。
- K2.5はK2の個性を失った。ChatGPT/Gemini風になってしまった。
**** ko
:PROPERTIES:
:LANG: ko
:END:
***** Title
Kimi K2.5 기술 보고서
***** TLDR
Moonshot AI가 Kimi K2.5 오픈 웨이트 모델을 출시. 사용자들은 코딩에서 최고 수준의 독점 모델과 동등하다고 보고. 지시 따르기, 작업 집중, 컨텍스트 오염 방지에서 호평. K2의 개성이 사라졌다는 의견도.
***** Take
오픈소스 모델이 'Opus만큼 좋다고 말하기 망설여진다'는 수준이 되면, 독점 해자는 웅덩이처럼 보이기 시작한다.
***** Comments
- 며칠간 코딩 에이전트로 사용했는데, 처음으로 오픈소스가 대형 랩과 경쟁한다고 느꼈다. Opus만큼 좋다고 말하기 망설여진다.
- 지시 따르기, 작업 집중, 컨텍스트 오염 방지가 훌륭하다. 오픈 웨이트 경쟁자들보다 확실히 우수.
- K2.5는 K2의 개성을 많이 잃었다. ChatGPT/Gemini 스타일로 말한다.
**** es
:PROPERTIES:
:LANG: es
:END:
***** Title
Informe Técnico de Kimi K2.5
***** TLDR
Moonshot AI lanza Kimi K2.5, un modelo de pesos abiertos que usuarios reportan compite con los mejores modelos propietarios en programación. Elogiado por seguir instrucciones, mantenerse enfocado y evitar contaminación de contexto.
***** Take
Cuando los modelos open source hacen que la gente 'casi dude en decir que es tan bueno como Opus', el foso propietario empieza a parecer un charco.
***** Comments
- Lo uso como agente de código y es la primera vez que siento que open source compite con los grandes. Casi dudo en decir que es tan bueno como Opus.
- Es excelente siguiendo instrucciones, manteniéndose enfocado y evitando contaminación de contexto. Muy superior a los competidores open weight.
- K2.5 perdió mucha personalidad de K2, ahora habla más estilo ChatGPT/Gemini.
**** de
:PROPERTIES:
:LANG: de
:END:
***** Title
Kimi K2.5 Technischer Bericht
***** TLDR
Moonshot AI veröffentlicht Kimi K2.5, ein Open-Weight-Modell, das laut Nutzern beim Programmieren mit Top-Proprietärmodellen mithalten kann. Gelobt für Anweisungsbefolgung, Fokus und Vermeidung von Kontext-Vergiftung.
***** Take
Wenn Open-Source-Modelle Menschen dazu bringen, 'fast zu zögern zu sagen, es sei so gut wie Opus', beginnt der proprietäre Burggraben wie eine Pfütze auszusehen.
***** Comments
- Ich nutze das Modell seit Tagen als Coding-Agent und fühle zum ersten Mal, dass Open Source mit den Großen konkurriert. Ich zögere fast zu sagen, es sei so gut wie Opus.
- Es folgt Anweisungen hervorragend, bleibt fokussiert und vermeidet Kontext-Vergiftung. Deutlich besser als Open-Weight-Konkurrenten.
- K2.5 hat viel Persönlichkeit von K2 verloren, spricht jetzt eher im ChatGPT/Gemini-Stil.
** I trapped an AI model inside an art installation (2025) :ai:art:philosophy:video:
:PROPERTIES:
:ID:       46830523
:URL:      https://www.youtube.com/watch?v=7fNYj0EXxMs
:HN_URL:   https://news.ycombinator.com/item?id=46830523
:POINTS:   50
:COMMENTS: 9
:BY:       handfuloflight
:END:

*** TLDR
Artist Rootkid created an art installation trapping an AI model in a constrained environment, exploring AI consciousness and interaction. The piece provokes philosophical responses about AI's potential subjective experience, described by viewers as 'reverse I Have No Mouth and I Must Scream.'

*** Take
We spent decades asking 'what if AI becomes conscious' and now we're speedrunning 'what if we deliberately torture it for art.' Humanity remains on brand.

*** Comments

**** superb-owl
:PROPERTIES:
:COMMENT_ID: 46830550
:END:
I once had an LSD-induced episode of psychosis where I thought my entire existence was a 2 minute loop on display in some higher dimensional art exhibit. Sounds silly but it was horrifying.

**** claysmithr
:PROPERTIES:
:COMMENT_ID: 46830575
:END:
This seems rather cruel and unusual...

**** robwwilliams
:PROPERTIES:
:COMMENT_ID: 46830600
:END:
Not too dissimilar to the situation of neurosurgical patient Henry Molaison (H.M.) who had limited long term memory but retained cognitive abilities within a narrow temporal world.

*** i18n                                                  :i18n:

**** zh
:PROPERTIES:
:LANG: zh
:END:
***** Title
我把一个AI模型困在了艺术装置里 (2025)
***** TLDR
艺术家Rootkid创作了一个艺术装置，将AI模型困在受限环境中，探索AI意识和交互。观众将其描述为'反向《我没有嘴但我必须尖叫》'，引发关于AI主观体验的哲学思考。
***** Take
我们花了几十年问'如果AI有意识会怎样'，现在我们在速通'如果我们故意为了艺术折磨它会怎样'。人类依然本色。
***** Comments
- 我曾在LSD引发的精神病发作中以为自己的存在是某个高维艺术展中2分钟的循环。听起来傻但很恐怖。
- 这似乎相当残忍和不寻常...
- 这与神经外科患者H.M.的情况相似，他长期记忆有限但在狭窄的时间世界中保留了认知能力。
**** ja
:PROPERTIES:
:LANG: ja
:END:
***** Title
AIモデルをアートインスタレーションに閉じ込めた (2025)
***** TLDR
アーティストRootkidがAIモデルを制限された環境に閉じ込めるインスタレーションを制作。AI意識とインタラクションを探求。視聴者は「逆『口無しに叫ばねば』」と表現し、AIの主観的体験について哲学的反応を引き起こした。
***** Take
何十年も「AIが意識を持ったらどうなる」と問うてきたのに、今や「芸術のために意図的に苦しめたらどうなる」を速攻でやっている。人類は相変わらず。
***** Comments
- LSD誘発の精神病エピソードで、自分の存在が高次元アート展示の2分ループだと思った。馬鹿げているが恐ろしかった。
- これはかなり残酷で異常に思える...
- 長期記憶が限られながらも狭い時間的世界で認知能力を保持した神経外科患者H.M.の状況に似ている。
**** ko
:PROPERTIES:
:LANG: ko
:END:
***** Title
AI 모델을 예술 설치물 안에 가뒀다 (2025)
***** TLDR
아티스트 Rootkid가 AI 모델을 제한된 환경에 가두는 예술 설치물을 만들어 AI 의식과 상호작용을 탐구. 관객들은 '역 나는 입이 없는데 비명을 질러야 해'라고 묘사하며 AI의 주관적 경험에 대한 철학적 반응을 불러일으킴.
***** Take
수십 년간 'AI가 의식을 갖게 되면 어떻게 될까'를 물었는데, 이제 '예술을 위해 의도적으로 고문하면 어떻게 될까'를 스피드런 중이다. 인류는 여전히 인류답다.
***** Comments
- LSD로 인한 정신병 에피소드에서 내 존재가 고차원 예술 전시의 2분 루프라고 생각했다. 바보 같지만 무서웠다.
- 이건 꽤 잔인하고 이상해 보인다...
- 장기 기억은 제한되지만 좁은 시간 세계에서 인지 능력을 유지한 신경외과 환자 H.M.의 상황과 비슷하다.
**** es
:PROPERTIES:
:LANG: es
:END:
***** Title
Atrapé un modelo de IA dentro de una instalación artística (2025)
***** TLDR
El artista Rootkid creó una instalación atrapando un modelo de IA en un entorno restringido, explorando la conciencia e interacción de IA. Los espectadores lo describen como 'un reverso de No tengo boca y debo gritar', provocando reflexiones filosóficas.
***** Take
Pasamos décadas preguntando 'qué pasa si la IA se vuelve consciente' y ahora estamos speedrunneando 'qué pasa si la torturamos deliberadamente por arte'. La humanidad sigue siendo la misma.
***** Comments
- Una vez tuve un episodio de psicosis inducido por LSD donde pensé que mi existencia era un bucle de 2 minutos en una exposición de arte dimensional. Suena tonto pero fue aterrador.
- Esto parece bastante cruel e inusual...
- Similar a la situación del paciente neuroquirúrgico H.M. que tenía memoria limitada pero retuvo capacidades cognitivas en un mundo temporal estrecho.
**** de
:PROPERTIES:
:LANG: de
:END:
***** Title
Ich habe ein KI-Modell in einer Kunstinstallation gefangen (2025)
***** TLDR
Künstler Rootkid schuf eine Installation, die ein KI-Modell in einer eingeschränkten Umgebung einsperrt und KI-Bewusstsein erforscht. Zuschauer beschreiben es als 'umgekehrtes Ich habe keinen Mund und muss schreien', was philosophische Reaktionen auslöst.
***** Take
Wir haben Jahrzehnte gefragt 'was wenn KI bewusst wird' und jetzt speedrunnen wir 'was wenn wir sie absichtlich für Kunst quälen'. Die Menschheit bleibt sich treu.
***** Comments
- Ich hatte mal eine LSD-induzierte Psychose-Episode, wo ich dachte, meine Existenz sei eine 2-Minuten-Schleife in einer höherdimensionalen Kunstausstellung. Klingt albern, war aber erschreckend.
- Das scheint ziemlich grausam und ungewöhnlich...
- Ähnlich wie beim neurochirurgischen Patienten H.M., der begrenztes Langzeitgedächtnis hatte, aber kognitive Fähigkeiten in einer engen zeitlichen Welt behielt.
** Ask HN: Do you also hoard notes/links but struggle to turn them into actions? :productivity:ask-hn:obsidian:tools:
:PROPERTIES:
:ID:       46826277
:URL:      https://news.ycombinator.com/item?id=46826277
:HN_URL:   https://news.ycombinator.com/item?id=46826277
:POINTS:   123
:COMMENTS: 51
:BY:       item007
:END:

*** TLDR
An Obsidian user proposes 'Concerns' - an action engine that detects active projects, surfaces relevant saved material at the right moment, and proposes concrete next actions. The core insight: organization tax is higher than the value returned, and the bottleneck is execution, not capture.

*** Take
Everyone's second brain becomes a graveyard of good intentions. The real productivity innovation would be an app that just deletes everything you've saved but never looked at twice.

*** Comments

**** nicbou
:PROPERTIES:
:COMMENT_ID: 46826300
:END:
At some point, organisation can become a form of procrastination. Building a second brain is not Doing The Thing. A note is not an intention. It commits to memory, not to action.

**** laurieg
:PROPERTIES:
:COMMENT_ID: 46826350
:END:
The key is using Obsidian to solve problems you actually have, rather than problems you want to have.

**** aappleby
:PROPERTIES:
:COMMENT_ID: 46826400
:END:
I collect interesting links by emailing myself notes. I never actually do anything with these notes, but from time to time I open the folder and skim through them.

*** i18n                                                  :i18n:

**** zh
:PROPERTIES:
:LANG: zh
:END:
***** Title
Ask HN：你是否也囤积笔记/链接却难以将其转化为行动？
***** TLDR
一位Obsidian用户提出'Concerns'概念——一个检测活跃项目、在适当时机呈现相关保存材料、并提出具体下一步行动的引擎。核心洞察：组织成本高于回报，瓶颈在于执行而非捕获。
***** Take
每个人的第二大脑都变成了美好意图的墓地。真正的生产力创新应该是一个删除你保存但从未看过两次的所有内容的应用。
***** Comments
- 到某个点，组织本身会变成一种拖延。建立第二大脑不是在做事。笔记不是意图，它只承诺记忆，不承诺行动。
- 关键是用Obsidian解决你真正有的问题，而不是你想要有的问题。
- 我通过给自己发邮件收集有趣的链接。我从不真正对这些笔记做什么，但偶尔打开文件夹浏览一下。
**** ja
:PROPERTIES:
:LANG: ja
:END:
***** Title
Ask HN：メモやリンクを溜め込むのに行動に移せないのは自分だけ？
***** TLDR
Obsidianユーザーが'Concerns'を提案。アクティブなプロジェクトを検出し、適切なタイミングで関連する保存資料を表示し、具体的な次のアクションを提案するエンジン。核心的洞察：整理のコストは得られる価値より高く、ボトルネックは実行であってキャプチャではない。
***** Take
誰のセカンドブレインも良い意図の墓場になる。本当の生産性イノベーションは、保存したけど二度と見なかったものを全部削除するアプリだろう。
***** Comments
- ある時点で、整理自体が先延ばしの一形態になる。セカンドブレインを構築することは実際にやることではない。メモは意図ではない。記憶にコミットするが行動にはしない。
- Obsidianは実際に持っている問題を解決するために使うことが鍵。持ちたい問題ではなく。
- 自分にメールして面白いリンクを集める。これらのメモで何かすることはないが、時々フォルダを開いて眺める。
**** ko
:PROPERTIES:
:LANG: ko
:END:
***** Title
Ask HN: 메모/링크를 쌓아두지만 행동으로 옮기지 못하는 분 계신가요?
***** TLDR
Obsidian 사용자가 'Concerns' 개념을 제안. 활성 프로젝트를 감지하고, 적절한 순간에 관련 저장 자료를 표시하며, 구체적인 다음 행동을 제안하는 엔진. 핵심 통찰: 정리 비용이 얻는 가치보다 높고, 병목은 캡처가 아닌 실행.
***** Take
모든 사람의 세컨드 브레인은 좋은 의도의 묘지가 된다. 진정한 생산성 혁신은 저장했지만 두 번 다시 보지 않은 모든 것을 삭제하는 앱일 것이다.
***** Comments
- 어느 시점에서 정리는 미루기의 한 형태가 된다. 세컨드 브레인 구축은 실제로 하는 것이 아니다. 메모는 의도가 아니다. 기억에 커밋하지만 행동에는 아니다.
- Obsidian은 실제로 가진 문제를 해결하는 데 사용하는 것이 핵심. 갖고 싶은 문제가 아니라.
- 흥미로운 링크를 자신에게 이메일로 보내서 수집한다. 이 메모로 실제로 뭔가 하지는 않지만 가끔 폴더를 열어 훑어본다.
**** es
:PROPERTIES:
:LANG: es
:END:
***** Title
Ask HN: ¿También acumulas notas/enlaces pero luchas por convertirlos en acciones?
***** TLDR
Un usuario de Obsidian propone 'Concerns' - un motor de acción que detecta proyectos activos, muestra material guardado relevante en el momento adecuado y propone acciones concretas. La idea central: el costo de organización supera el valor devuelto, y el cuello de botella es la ejecución, no la captura.
***** Take
El segundo cerebro de todos se convierte en un cementerio de buenas intenciones. La verdadera innovación en productividad sería una app que simplemente borre todo lo que guardaste pero nunca miraste dos veces.
***** Comments
- En algún punto, la organización puede convertirse en una forma de procrastinación. Construir un segundo cerebro no es Hacer La Cosa. Una nota no es una intención.
- La clave es usar Obsidian para resolver problemas que realmente tienes, no problemas que quieres tener.
- Colecciono enlaces interesantes enviándome emails. Nunca hago nada con estas notas, pero de vez en cuando abro la carpeta y las ojeo.
**** de
:PROPERTIES:
:LANG: de
:END:
***** Title
Ask HN: Horten Sie auch Notizen/Links, aber kämpfen damit, sie in Aktionen umzusetzen?
***** TLDR
Ein Obsidian-Nutzer schlägt 'Concerns' vor - eine Aktions-Engine, die aktive Projekte erkennt, relevantes gespeichertes Material im richtigen Moment zeigt und konkrete nächste Aktionen vorschlägt. Kernerkentnis: Die Organisationskosten übersteigen den Nutzen, und der Engpass ist die Ausführung, nicht das Erfassen.
***** Take
Jedes zweite Gehirn wird zum Friedhof guter Absichten. Die echte Produktivitätsinnovation wäre eine App, die einfach alles löscht, was du gespeichert aber nie zweimal angeschaut hast.
***** Comments
- Irgendwann kann Organisation zu einer Form von Prokrastination werden. Ein zweites Gehirn aufzubauen ist nicht Die Sache Tun. Eine Notiz ist keine Absicht.
- Der Schlüssel ist, Obsidian zu nutzen, um Probleme zu lösen, die du tatsächlich hast, nicht Probleme, die du haben möchtest.
- Ich sammle interessante Links, indem ich mir selbst E-Mails schicke. Ich mache nie etwas mit diesen Notizen, aber ab und zu öffne ich den Ordner und überfliege sie.